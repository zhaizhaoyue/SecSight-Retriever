import os
import json
import gzip
import requests
from argparse import ArgumentParser
from datasets import load_dataset
from datasets.exceptions import DatasetGenerationError
#####    python src/data/data_get.py --mode download     #####
BASE_DIR = "data/testing_data"

# å®šä¹‰æ•°æ®é›†åŠå…¶ç›®æ ‡ç›®å½•
DATASETS = {
    # domain: {dataset_name: (hf_id, backup_hf_id_or_None)}
    "finance": {
        "docfinqa": ("kensho/DocFinQA", None),
        "tatqa": ("next-tat/TAT-QA", "ibm/TAT-QA"),  # å¤‡é€‰æº
        "finqa": ("dreamerdeo/finqa", None),
        "convfinqa": ("ChilleD/ConvFinQA", None),
        "multihiertt": ("microsoft/MultiHiertt", None),
        "finer": ("nlpaueb/finer-139", None),
    },
    "government": {
        "govreport": ("launch/gov_report", "ccdv/govreport-summarization"),
        "qmsum": ("pszemraj/qmsum-cleaned", None),
        "billsum": ("FiscalNote/BillSum", None),
        "multinews": ("multi_news", None),
        "cnn_dailymail": ("cnn_dailymail", None),
    },
    "science_law": {
        "pubmedqa": ("bigbio/pubmed_qa", "pubmed_qa"),
        "cuad": ("theatticusproject/cuad", None),
        "scifact": ("scifact", None),
        "evidence_infer": ("evidence_infer_treatment", None),
        "legal_pile": ("pile-of-law/pile-of-law", None),
        "case_hold": ("casehold", None),
    },
    "general_qa": {
        "squad": ("squad", None),
        "squad_v2": ("squad_v2", None),
        "natural_questions": ("natural_questions", None),
        "ms_marco": ("ms_marco", None),
        "hotpotqa": ("hotpot_qa", None),
        "narrativeqa": ("narrativeqa", None),
    },
    "summarization": {
        "xsum": ("xsum", None),
        "reddit_tifu": ("reddit_tifu", None),
        "booksum": ("kmfoda/booksum", None),
        "arxiv": ("scientific_papers", None),
        "pubmed": ("scientific_papers", None),
    },
}

def save_split(dataset, split_name, out_path, domain, dataset_name, compress=False):
    """ä¿å­˜å•ä¸ª split ä¸º JSONLï¼ˆå¯é€‰å‹ç¼©ï¼‰"""
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    opener = gzip.open if compress else open
    mode = "wt" if compress else "w"
    
    with opener(out_path, mode, encoding="utf-8") as f:
        for i, item in enumerate(dataset):
            record = build_record_safe(item, dataset_name, split_name, i, domain)
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

def build_record_safe(item, dataset_name, split_name, idx, domain):
    """æŒ‰ç…§ AIE Pipeline é¢„æœŸæ ¼å¼æ„é€ è®°å½•ï¼Œå¹¶å¯¹ä¸å¯åºåˆ—åŒ–å¯¹è±¡åšå®‰å…¨å¤„ç†ã€‚

    è¾“å‡ºå­—æ®µï¼š
    - document_id: å”¯ä¸€æ ·æœ¬IDï¼ˆå­—ç¬¦ä¸²ï¼‰
    - document_text: æ–‡æœ¬å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
    - query: ç”¨æˆ·é—®é¢˜/æŸ¥è¯¢ï¼ˆå­—ç¬¦ä¸²ï¼Œå¯ä¸ºç©ºï¼‰
    - metadata: å…¶ä»–å…ƒä¿¡æ¯ï¼ˆå­—å…¸ï¼‰ï¼ŒåŒ…å« domain/dataset/split/answers/åŸå§‹å‰©ä½™å­—æ®µ
    """
    def to_json_safe(obj):
        # åŸºç¡€ç±»å‹ç›´æ¥è¿”å›
        if obj is None or isinstance(obj, (bool, int, float, str)):
            return obj
        # bytes â†’ å°è¯•utf-8
        if isinstance(obj, (bytes, bytearray)):
            try:
                return obj.decode("utf-8", errors="ignore")
            except Exception:
                return None
        # åˆ—è¡¨ / å…ƒç»„
        if isinstance(obj, (list, tuple)):
            return [to_json_safe(x) for x in obj]
        # å­—å…¸
        if isinstance(obj, dict):
            return {str(k): to_json_safe(v) for k, v in obj.items()}
        # å…¶ä»–å¯¹è±¡ â†’ å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œé˜²æ­¢PDFç­‰å¯¹è±¡æŠ¥é”™
        try:
            s = str(obj)
            # é™åˆ¶å­—ç¬¦ä¸²è¿‡é•¿
            if len(s) > 200000:
                return s[:200000] + "â€¦"
            return s
        except Exception:
            return None

    # å¸¸è§å­—æ®µæŠ½å–ï¼ˆæ‰©å±•å€™é€‰ï¼Œå…¼å®¹ä¸åŒæ•°æ®é›†ï¼‰
    doc_candidates = [
        item.get("context"), item.get("document"), item.get("text"),
        item.get("input"), item.get("passage"), item.get("dialogue"),
        item.get("transcript"), item.get("article"), item.get("content"),
    ]
    document = next((d for d in doc_candidates if isinstance(d, (str, list, tuple)) and d), "")
    if isinstance(document, (list, tuple)):
        document = "\n".join([str(x) for x in document])
    question = item.get("question") or item.get("query") or item.get("instruction") or None
    answers = item.get("answers") or item.get("answer") or []

    # å…ƒæ•°æ®ï¼šæ’é™¤å·²æå–å­—æ®µï¼Œéšåä¸ domain/dataset/split/answers åˆå¹¶
    metadata_raw = {k: v for k, v in item.items() if k not in ["context", "document", "text", "question", "query", "answers", "answer", "input", "passage", "dialogue", "transcript", "article", "content"]}

    aie_record = {
        "document_id": f"{dataset_name}_{split_name}_{idx}",
        "document_text": to_json_safe(document) or "",
        "query": (to_json_safe(question) or ""),
        "metadata": to_json_safe({
            "domain": domain,
            "dataset": dataset_name,
            "split": split_name,
            "answers": to_json_safe(answers),
            **metadata_raw,
        }),
    }
    return aie_record

def load_raw_json_dataset(dataset_id: str, file_urls: dict):
    """ç›´æ¥ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼Œç»•è¿‡HuggingFaceçš„Arrowè½¬æ¢"""
    import io

    class RawJSONDataset:
        def __init__(self, splits_data):
            self.splits_data = splits_data
        def keys(self):
            return self.splits_data.keys()
        def __getitem__(self, split):
            return self.splits_data[split]

    splits_data = {}
    for split, url in file_urls.items():
        try:
            print(f"  ğŸ“¥ Downloading {split} from raw JSON...")
            if url.startswith("hf://"):
                # è½¬æ¢HF URLä¸ºç›´æ¥ä¸‹è½½é“¾æ¥
                url = url.replace("hf://datasets/", "https://huggingface.co/datasets/").replace("@", "/resolve/") + "?download=true"

            # âœ… æµå¼ + è¶…æ—¶
            with requests.get(url, stream=True, timeout=60) as r:
                r.raise_for_status()
                buf = io.StringIO()
                for chunk in r.iter_content(chunk_size=1 << 16):
                    if chunk:
                        buf.write(chunk.decode("utf-8", errors="ignore"))
                content = buf.getvalue()

            data = []
            # JSONL æƒ…å†µ
            if "\n" in content and not content.strip().startswith('['):
                for line in content.strip().splitlines():
                    line = line.strip()
                    if line:
                        try:
                            data.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
            else:
                # æ ‡å‡† JSONï¼ˆlist æˆ– dictï¼‰
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, list):
                        data = parsed
                    elif isinstance(parsed, dict):
                        data = [parsed]
                except json.JSONDecodeError as e:
                    print(f"  âŒ JSON decode error for {split}: {e}")
                    continue

            splits_data[split] = data
            print(f"  âœ… Loaded {len(data)} samples for {split}")

        except Exception as e:
            print(f"  âŒ Error loading {split}: {e}")
            continue

    return RawJSONDataset(splits_data) if splits_data else None

def robust_load_dataset(hf_id: str, backup_id: str = None):
    """å°½é‡å¥å£®åœ°åŠ è½½HFæ•°æ®é›†ï¼š
    - TAT-QAï¼šå¼ºåˆ¶èµ°åŸå§‹ JSONï¼Œå½»åº•ç»•è¿‡ Arrow
    - PubMedQAï¼šæŒ‰å¸¸è§ config è½®è¯¢åŠ è½½ï¼ˆbigbio ä¸åŸç‰ˆéƒ½è¯•ï¼‰
    - å…¶ä½™ï¼šä¸»æº â†’ å¤‡é€‰æº â†’ streaming/ignore_verifications/no_checks ç­‰å…œåº•
    """

    # === ç‰¹åˆ¤ 1ï¼šTAT-QA ç›´æ¥æ‹‰åŸå§‹ JSONï¼Œé¿å… ArrowInvalid ===
    if hf_id in ["next-tat/TAT-QA", "ibm/TAT-QA"]:
        print(f"  ğŸ”„ Forcing direct JSON loading for {hf_id}")
        file_urls = {
            "train": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_train.json",
            "validation": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_dev.json",
            "test": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_test_gold.json",
        }
        raw = load_raw_json_dataset(hf_id, file_urls)
        if raw is None:
            raise Exception("TAT-QA raw JSON loading failed")
        return raw

    # === ç‰¹åˆ¤ 2ï¼šPubMedQA é…ç½®è½®è¯¢ï¼ˆæ–¹æ¡ˆ Aï¼‰ ===
    if hf_id in ["bigbio/pubmed_qa", "pubmed_qa"]:
        print(f"  ğŸ”„ Trying PubMedQA with common configs for {hf_id}")
        # bigbio ç‰ˆæœ¬ï¼ˆä»»åŠ¡é€šå¸¸æ˜¯ bigbio_qaï¼‰
        bigbio_configs = [
            "pubmed_qa_pqa_labeled_bigbio_qa",
            "pubmed_qa_pqa_artificial_bigbio_qa",
            "pubmed_qa_pqa_unlabeled_bigbio_qa",
        ]
        # åŸç‰ˆ PubMedQA çš„å¸¸è§é…ç½®
        vanilla_configs = [
            "pqa_labeled",
            "pqa_artificial",
            "pqa_unlabeled",
        ]
        cfg_list = bigbio_configs if hf_id == "bigbio/pubmed_qa" else vanilla_configs

        # å…ˆè¯•å½“å‰ hf_id çš„æ‰€æœ‰ config
        for cfg in cfg_list:
            try:
                print(f"    â€¢ trying config: {cfg}")
                ds = load_dataset(hf_id, cfg)
                print(f"    âœ… loaded: {hf_id} ({cfg})")
                return ds
            except Exception as e:
                print(f"    âœ— failed: {cfg} ({e.__class__.__name__})")

        # è‹¥ bigbio å…¨å¤±è´¥ï¼Œå›é€€åˆ°åŸç‰ˆ pubmed_qa çš„ configs
        if hf_id == "bigbio/pubmed_qa":
            for cfg in vanilla_configs:
                try:
                    print(f"    â€¢ trying fallback hf_id=pubmed_qa config: {cfg}")
                    ds = load_dataset("pubmed_qa", cfg)
                    print(f"    âœ… loaded: pubmed_qa ({cfg})")
                    return ds
                except Exception:
                    pass

        raise Exception("PubMedQA: all known configs failed")

    # === å…¶ä½™æ•°æ®é›†ï¼šåŸå…œåº•é€»è¾‘ ===
    def try_load_single(dataset_id: str):
        # scientific_papers éœ€è¦æ˜¾å¼ configï¼ˆæ ¹æ® hf_id æ–‡æœ¬çŒœæµ‹ï¼‰
        config_name = None
        if dataset_id == "scientific_papers":
            if "arxiv" in (hf_id or "").lower():
                config_name = "arxiv"
            elif "pubmed" in (hf_id or "").lower():
                config_name = "pubmed"

        # 1) æ­£å¸¸åŠ è½½
        try:
            if config_name:
                return load_dataset(dataset_id, config_name)
            else:
                return load_dataset(dataset_id)
        except (DatasetGenerationError, ValueError, Exception) as e:
            # CUAD ç¼ºå°‘ pdfplumber çš„æ˜ç¡®æç¤º
            if "theatticusproject/cuad" in dataset_id and "pdfplumber" in str(e).lower():
                raise Exception("CUAD éœ€è¦ pdfplumberï¼šè¯·å…ˆ `pip install pdfplumber` ç„¶åé‡è¯•") from e
            pass

        # 2) æµå¼åŠ è½½ï¼ˆç»•è¿‡ Arrow æ„å»ºï¼‰
        try:
            if config_name:
                return load_dataset(dataset_id, config_name, streaming=True)
            else:
                return load_dataset(dataset_id, streaming=True)
        except (DatasetGenerationError, ValueError, Exception):
            pass

        # 2.5) æµå¼ + å¿½ç•¥æ ¡éªŒ
        try:
            if config_name:
                return load_dataset(dataset_id, config_name, streaming=True, ignore_verifications=True)
            else:
                return load_dataset(dataset_id, streaming=True, ignore_verifications=True)
        except (DatasetGenerationError, ValueError, Exception):
            pass

        # 3) å…³é—­æ ¡éªŒ
        try:
            if config_name:
                return load_dataset(dataset_id, config_name, verification_mode="no_checks")
            else:
                return load_dataset(dataset_id, verification_mode="no_checks")
        except Exception:
            pass

        return None

    # ä¸»æº
    result = try_load_single(hf_id)
    if result is not None:
        return result

    # å¤‡é€‰æº
    if backup_id:
        print(f"  âš ï¸  Main source failed, trying backup: {backup_id}")
        result = try_load_single(backup_id)
        if result is not None:
            return result

    # éƒ½å¤±è´¥
    raise Exception(f"Failed to load both {hf_id} and {backup_id or 'no backup'}")



def main():
    parser = ArgumentParser(description="Download datasets to JSONL")
    parser.add_argument("--domain", default=None, help="Filter by domain (finance, government, etc.)")
    parser.add_argument("--dataset", default=None, help="Filter by specific dataset name")
    parser.add_argument("--split", default=None, help="Filter by split (train, validation, test)")
    parser.add_argument("--skip-existing", action="store_true", help="Skip datasets that already exist")
    parser.add_argument("--compress", action="store_true", help="Compress output files with gzip (saves ~70% space but slower to read)")
    parser.add_argument("--list-datasets", action="store_true", help="List all available datasets and exit")
    args = parser.parse_args()

    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ•°æ®é›†
    if args.list_datasets:
        print("ğŸ“‹ Available datasets:")
        total_datasets = 0
        for domain, datasets in DATASETS.items():
            print(f"\nğŸ·ï¸  {domain.upper()}:")
            for dataset_name, (hf_id, backup_id) in datasets.items():
                backup_info = f" (å¤‡é€‰: {backup_id})" if backup_id else ""
                print(f"   â€¢ {dataset_name}: {hf_id}{backup_info}")
                total_datasets += 1
        print(f"\nğŸ“Š Total: {total_datasets} datasets across {len(DATASETS)} domains")
        return

    # ä¸‹è½½æ•°æ®é›†
    success_count = 0
    error_count = 0
    for domain, datasets in DATASETS.items():
        if args.domain and domain != args.domain:
            continue
        for dataset_name, (hf_id, backup_id) in datasets.items():
            if args.dataset and dataset_name != args.dataset:
                continue
            print(f"Loading {hf_id} â†’ {domain}/{dataset_name}")
            if backup_id:
                print(f"  (å¤‡é€‰æº: {backup_id})")
            try:
                ds = robust_load_dataset(hf_id, backup_id)
                split_names = list(ds.keys()) if hasattr(ds, "keys") else list(ds)
                for split in split_names:
                    if args.split and split != args.split:
                        continue
                    out_path = os.path.join(
                        BASE_DIR, domain, dataset_name, f"{split}.jsonl"
                    )
                    if args.compress:
                        out_path += ".gz"
                    
                    # æ£€æŸ¥æ˜¯å¦è·³è¿‡ç°æœ‰æ–‡ä»¶
                    if args.skip_existing and os.path.exists(out_path):
                        print(f"  â­ï¸  Skipping existing split {split}")
                        continue
                        
                    print(f"  Saving split {split} â†’ {out_path}")
                    try:
                        save_split(ds[split], split, out_path, domain, dataset_name, args.compress)
                        print(f"  âœ… Successfully saved {split} split")
                    except Exception as e:
                        print(f"  âŒ Error saving split {split}: {e}")
                        # å¯¹äºArrowè½¬æ¢é”™è¯¯ï¼Œå°è¯•ç›´æ¥JSONåŠ è½½
                        if "Arrow" in str(e) and hf_id in ["next-tat/TAT-QA", "ibm/TAT-QA"]:
                            print(f"  ğŸ”„ Retrying with direct JSON loading for {split}")
                            try:
                                file_urls = {
                                    "train": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_train.json",
                                    "validation": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_dev.json",
                                    "test": "https://huggingface.co/datasets/next-tat/TAT-QA/resolve/main/tatqa_dataset_test_gold.json"
                                }
                                raw_ds = load_raw_json_dataset(hf_id, {split: file_urls[split]})
                                if raw_ds and split in raw_ds.keys():
                                    save_split(raw_ds[split], split, out_path, domain, dataset_name, args.compress)
                                    print(f"  âœ… Successfully saved {split} split (via raw JSON)")
                                    continue
                            except Exception as e2:
                                print(f"  âŒ Raw JSON loading also failed: {e2}")
                        error_count += 1
                        continue
                success_count += 1
                print(f"âœ… Successfully processed {dataset_name}")
            except Exception as e:
                print(f"âŒ Error loading dataset {hf_id}: {e}")
                print(f"   Skipping {dataset_name} and continuing...")
                error_count += 1
                continue
        print(f"\nğŸ“Š Summary:")
        print(f"   âœ… Success: {success_count} datasets")
        print(f"   âŒ Errors: {error_count} datasets")
        if error_count > 0:
            print(f"   âš ï¸  Some datasets failed to load - this is normal for problematic datasets")

if __name__ == "__main__":
    main()
