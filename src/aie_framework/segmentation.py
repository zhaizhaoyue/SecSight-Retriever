"""
Document Segmentation Module - æ–‡æ¡£åˆ†æ®µæ¨¡å—

ğŸ¯ åŠŸèƒ½ï¼šå°†é•¿æ–‡æ¡£æ™ºèƒ½åˆ†å‰²ä¸ºå¯ç®¡ç†çš„ç‰‡æ®µ
ğŸ“ æ”¯æŒï¼šå›ºå®šé•¿åº¦ã€è¯­ä¹‰ç›¸ä¼¼åº¦ã€æ··åˆåˆ†æ®µç­‰å¤šç§ç­–ç•¥
ğŸš€ ç‰¹æ€§ï¼šCUDAåŠ é€Ÿçš„è¯­ä¹‰åˆ†æ®µï¼Œè‡ªåŠ¨å†…å®¹ç±»å‹è¯†åˆ«
ğŸ’¡ ç”¨é€”ï¼šé•¿ç¯‡æ–‡æ¡£é¢„å¤„ç†ï¼Œä¸ºæ£€ç´¢å’Œåˆ†ææä¾›åŸºç¡€

æ ¸å¿ƒç±»ï¼š
- DocumentSegmenter: ä¸»åˆ†æ®µå™¨ï¼Œæ ¹æ®é…ç½®é€‰æ‹©åˆ†æ®µç­–ç•¥
- FixedLengthSegmenter: å›ºå®šé•¿åº¦åˆ†æ®µï¼Œé€‚åˆå¿«é€Ÿå¤„ç†
- SemanticSegmenter: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½åˆ†æ®µ (CUDAåŠ é€Ÿ)
- HybridSegmenter: æ··åˆåˆ†æ®µï¼Œæ”¯æŒè¡¨æ ¼ã€å›¾åƒã€æ ‡é¢˜è¯†åˆ«

å…¸å‹ç”¨æ³•ï¼š
# 1) é»˜è®¤é€’å½’ data/**/train.jsonlï¼Œè¾“å‡ºåˆ° data/segmented_data é•œåƒ
python segmentation.py

# 2) æŒ‡å®šåˆ†æ®µç­–ç•¥ä¸é•¿åº¦ï¼ˆhybrid + 900 è¿‘ä¼¼ tokenï¼Œ10% é‡å ï¼‰
python segmentation.py --split-method hybrid --max-segment-length 900 --overlap-ratio 0.1

# 3) å¼€å¯ semantic æ–­ç‚¹ï¼ˆéœ€è¦ sentence-transformersï¼‰
python segmentation.py --split-method hybrid --semantic-breaks --semantic-device cuda

# 4) è‡ªå®šä¹‰æ–‡æœ¬å­—æ®µå€™é€‰ï¼ˆæŒ‰é¡ºåºåŒ¹é…ï¼‰
python segmentation.py --text-keys "document,text,content,report"

# 5) è¾“å‡ºä¸­ä¿ç•™åŸå§‹è¾“å…¥è®°å½•ï¼ˆä¾¿äºå¯¹é½/è°ƒè¯•ï¼‰
python segmentation.py --keep-input
# 6) è¿½æ±‚ç²¾å‡†åº¦:
# ä½¿ç”¨ Hybrid åˆ†æ®µå™¨ + è¯­ä¹‰æ–­ç‚¹ï¼ˆéœ€è¦ GPU æˆ– CPU æ”¯æŒ sentence-transformersï¼‰
python -m src.aie_framework.segmentation `
  --split-method hybrid `
  --max-segment-length 800 `
  --overlap-ratio 0.05 `
  --data-root "data/testing_data" `
  --out-root "data/segmented_data" `
  --glob-name "train.jsonl" `

"""

import re
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import os, json, argparse
from pathlib import Path
import numpy as np
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)


@dataclass
class DocumentSegment:
    """Document segment data class"""
    id: str
    content: str
    segment_type: str  # text, table, image, header
    start_pos: int
    end_pos: int
    metadata: Dict[str, Any]
    
    def __len__(self) -> int:
        return len(self.content)
    
    def __str__(self) -> str:
        return f"Segment({self.id}, {self.segment_type}, {len(self.content)} chars)"


class BaseSegmenter(ABC):
    """Base segmenter class"""
    
    @abstractmethod
    def segment(self, document: str, metadata: Optional[Dict] = None) -> List[DocumentSegment]:
        """Segmentation method"""
        pass


class FixedLengthSegmenter(BaseSegmenter):
    """å›ºå®šlengthåˆ†æ®µå™¨"""
    
    def __init__(self, max_length: int = 1000, overlap_ratio: float = 0.1):
        self.max_length = max_length
        self.overlap_size = int(max_length * overlap_ratio)
        
    def segment(self, document: str, metadata: Optional[Dict] = None) -> List[DocumentSegment]:
        """Segment by fixed length"""
        segments = []
        start = 0
        segment_id = 0
        
        while start < len(document):
            end = min(start + self.max_length, len(document))
            
            # å°è¯•åœ¨åˆé€‚çš„ä½ç½®åˆ†å‰²ï¼ˆå¥å·ã€æ¢è¡Œç­‰ï¼‰
            if end < len(document):
                # å‘åæŸ¥æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹
                for i in range(end, max(start, end - 100), -1):
                    if document[i] in '.!?\n':
                        end = i + 1
                        break
            
            content = document[start:end].strip()
            if content:
                segment = DocumentSegment(
                    id=f"seg_{segment_id:04d}",
                    content=content,
                    segment_type="text",
                    start_pos=start,
                    end_pos=end,
                    metadata=metadata or {}
                )
                segments.append(segment)
                segment_id += 1
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªå¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(start + 1, end - self.overlap_size)
            
        return segments


class SemanticSegmenter(BaseSegmenter):
    """è¯­ä¹‰åˆ†æ®µå™¨ï¼ŒåŸºäºå¥å­åµŒå…¥çš„ç›¸ä¼¼åº¦"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2", 
                 max_length: int = 1000, similarity_threshold: float = 0.7, device: str = "cuda"):
        self.max_length = max_length
        self.similarity_threshold = similarity_threshold
        self.device = device
        self.sentence_model = SentenceTransformer(model_name, device=device)
        
    def _split_into_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­"""
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•å¦‚spaCyï¼‰
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _calculate_similarity(self, sent1: str, sent2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦"""
        embeddings = self.sentence_model.encode([sent1, sent2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (
            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
        )
        return similarity
    
    def segment(self, document: str, metadata: Optional[Dict] = None) -> List[DocumentSegment]:
        """Segment based on semantic similarity"""
        sentences = self._split_into_sentences(document)
        if not sentences:
            return []
        
        segments = []
        current_segment = [sentences[0]]
        segment_id = 0
        current_length = len(sentences[0])
        
        for i in range(1, len(sentences)):
            sentence = sentences[i]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°æ®µè½
            should_start_new = False
            
            # lengthæ£€æŸ¥
            if current_length + len(sentence) > self.max_length:
                should_start_new = True
            else:
                # è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥
                last_sentence = current_segment[-1]
                similarity = self._calculate_similarity(last_sentence, sentence)
                if similarity < self.similarity_threshold:
                    should_start_new = True
            
            if should_start_new and current_segment:
                # åˆ›å»ºå½“å‰æ®µè½
                segment_text = ' '.join(current_segment)
                start_pos = document.find(current_segment[0])
                end_pos = start_pos + len(segment_text)
                
                segment = DocumentSegment(
                    id=f"sem_seg_{segment_id:04d}",
                    content=segment_text,
                    segment_type="text",
                    start_pos=start_pos,
                    end_pos=end_pos,
                    metadata=metadata or {}
                )
                segments.append(segment)
                
                # å¼€å§‹æ–°æ®µè½
                current_segment = [sentence]
                current_length = len(sentence)
                segment_id += 1
            else:
                current_segment.append(sentence)
                current_length += len(sentence)
        
        # processingæœ€åä¸€ä¸ªæ®µè½
        if current_segment:
            segment_text = ' '.join(current_segment)
            start_pos = document.find(current_segment[0])
            end_pos = start_pos + len(segment_text)
            
            segment = DocumentSegment(
                id=f"sem_seg_{segment_id:04d}",
                content=segment_text,
                segment_type="text",
                start_pos=start_pos,
                end_pos=end_pos,
                metadata=metadata or {}
            )
            segments.append(segment)
        
        return segments


class HybridSegmenter(BaseSegmenter):
    """
    AIE-style hybrid segmenter with:
      1) Serialization (PLAIN for tables)
      2) Split   (long elements -> chunks)
      3) Merge   (adjacent short elements -> concat)
    Also tracks markdown headers and injects section_path into metadata.
    """

    def __init__(
        self,
        max_length: int = 1000,              # è¿‘ä¼¼ token ä¸Šé™ï¼ˆå­—ç¬¦/è¯è¿‘ä¼¼ï¼‰
        overlap_ratio: float = 0.1,          # æ–‡æœ¬åˆ‡ç‰‡é‡å 
        min_merge_tokens: int = 180,         # è¿‡çŸ­æ®µåˆå¹¶é˜ˆå€¼
        max_table_rows_per_chunk: int = 30,  # è¡¨æ ¼è¡Œå—å¤§å°ï¼ˆå«è¡¨å¤´ï¼‰
        add_section_headers: bool = True,    # å°†ä¸Šæ¸¸æ ‡é¢˜æ³¨å…¥ metadata
        semantic_breaks: bool = False,       # å¯¹æ–‡æœ¬åˆ‡ç‰‡å¯ç”¨è¯­ä¹‰æ–­ç‚¹
        semantic_model: str = "sentence-transformers/all-MiniLM-L6-v2",
        semantic_device: str = "cuda",
    ):
        self.max_length = max_length
        self.overlap_ratio = overlap_ratio
        self.overlap_size = max(0, int(max_length * overlap_ratio))
        self.min_merge_tokens = min_merge_tokens
        self.max_table_rows_per_chunk = max_table_rows_per_chunk
        self.add_section_headers = add_section_headers
        self.semantic_breaks = semantic_breaks

        self._sentence_model = None
        if semantic_breaks:
            try:
                self._sentence_model = SentenceTransformer(semantic_model, device=semantic_device)
            except Exception:
                logger.warning("SentenceTransformer init failed; semantic_breaks disabled.")
                self.semantic_breaks = False

    # ---------- low-level utils ----------
    _sent_pat = re.compile(r'(?<=[ã€‚ï¼.!?ï¼Ÿ!])\s+|[\r\n]+')
    _header_pat = re.compile(r'^(#{1,6})\s+(.*)$')
    _html_table_pat = re.compile(r'<table.*?>.*?</table>', re.S | re.I)

    @staticmethod
    def _token_len(text: str) -> int:
        # ç®€å•è¿‘ä¼¼ï¼šè¯+æ ‡ç‚¹ã€‚éœ€è¦æ›´å‡†å¯æ›¿æ¢ä¸º tiktokenã€‚
        return len(re.findall(r"\w+|[^\s\w]", text))

    def _split_text_by_tokens(self, text: str) -> List[str]:
        if self._token_len(text) <= self.max_length:
            return [text.strip()]

        # å…ˆå¥åˆ‡ï¼Œå†æŒ‰è¿‘ä¼¼ token æ§åˆ¶çª—å£ï¼Œå¿…è¦æ—¶æ·»åŠ é‡å 
        sents = [s.strip() for s in re.split(self._sent_pat, text) if s.strip()]
        chunks, cur, cur_len = [], [], 0

        def flush():
            if not cur: return
            chunk = " ".join(cur).strip()
            if chunk:
                chunks.append(chunk)

        for i, s in enumerate(sents):
            s_len = self._token_len(s)
            need_new = (cur_len + s_len > self.max_length)

            if self.semantic_breaks and cur:
                # è¯­ä¹‰æ–­ç‚¹ï¼šè‹¥ä¸ä¸Šä¸€å¥ç›¸ä¼¼åº¦è¿‡ä½ä¹Ÿæ¢æ®µ
                try:
                    emb = self._sentence_model.encode([cur[-1], s])
                    sim = float(np.dot(emb[0], emb[1]) /
                                (np.linalg.norm(emb[0]) * np.linalg.norm(emb[1]) + 1e-8))
                    if sim < 0.55:  # ç»éªŒé˜ˆå€¼ï¼Œå¯å…¥å‚
                        need_new = True
                except Exception:
                    pass

            if need_new:
                flush()
                # æ„é€ é‡å ï¼šæŠŠä¸Šæ®µæœ«å°¾è‹¥å¹² token ä½œä¸ºå¼€å¤´
                if self.overlap_size > 0 and chunks:
                    tail = cur[-1] if cur else ""
                    tail_tokens = tail.split()
                    overlap = " ".join(tail_tokens[-min(len(tail_tokens), self.overlap_size):])
                    cur, cur_len = ([overlap] if overlap else []), self._token_len(overlap)
                else:
                    cur, cur_len = [], 0

            cur.append(s)
            cur_len += s_len

        flush()
        return chunks

    @staticmethod
    def _parse_md_table(block: str) -> Optional[Dict[str, Any]]:
        """Parse a markdown table block to rows/cols; return None if not a table."""
        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]
        if len(lines) < 2 or not any("|" in ln for ln in lines[:2]):
            return None

        # å»é™¤é¦–å°¾ç«–çº¿ï¼Œåˆ†åˆ—
        rows = []
        for ln in lines:
            if set(ln) <= {"-", "|", ":", " "}:
                # å¯¹é½åˆ†éš”è¡Œï¼Œè·³è¿‡
                continue
            cells = [c.strip() for c in ln.strip("|").split("|")]
            rows.append(cells)

        if not rows:
            return None

        header = rows[0]
        data = rows[1:] if len(rows) > 1 else []
        # å¯¹é½åˆ—æ•°
        width = max(len(r) for r in [header] + data)
        header += ["col_%d" % i for i in range(len(header), width)]
        data = [r + [""] * (width - len(r)) for r in data]

        return {"header": header, "rows": data}

    @staticmethod
    def _serialize_plain_table(tbl: Dict[str, Any], title: Optional[str] = None) -> List[str]:
        """PLAIN: each row as 'col1=val1; col2=val2; ...' (optionally with row label)."""
        header = tbl["header"]
        lines = []
        if title:
            lines.append(f"[Table] {title}")

        for ridx, row in enumerate(tbl["rows"]):
            kvs = [f"{h}={v}" for h, v in zip(header, row)]
            line = "; ".join(kvs)
            lines.append(line)
        return lines

    def _detect_content_type(self, line: str) -> str:
        if "|" in line and not line.lstrip().startswith(("#", ">")):
            return "table"
        if self._html_table_pat.search(line):
            return "table"
        m = self._header_pat.match(line)
        if m:
            return "header"
        if re.search(r'\[å›¾\d+\]|\[å›¾åƒ\]|<img', line, re.I):
            return "image"
        return "text"

    def _split_by_structure(self, document: str) -> List[Tuple[str, str, Dict[str, Any], int, int]]:
        lines = document.splitlines()
        sections = []
        buf, cur_type = [], "text"
        pos = 0
        idx = 0

        section_stack: List[Tuple[int, str]] = []  # (level, title)

        def current_section_path() -> List[str]:
            return [t for _, t in section_stack]

        def flush():
            nonlocal buf, cur_type, idx, pos
            if not buf:
                return
            raw_block = "\n".join(buf)                # ç”¨ raw_block ç®— end
            content = raw_block.strip()
            if content:
                meta = {"section_path": current_section_path()} if self.add_section_headers else {}
                start = document.find(buf[0], pos)
                end = start + len(raw_block)          # âš ï¸ end ç”¨æœª strip çš„é•¿åº¦
                sections.append((content, cur_type, meta, start, end))
                idx += 1
                pos = end + 1
            buf = []

        i = 0
        while i < len(lines):
            ln = lines[i]
            m = self._header_pat.match(ln)
            if m:
                flush()
                level = len(m.group(1)); title = m.group(2).strip()
                while section_stack and section_stack[-1][0] >= level:
                    section_stack.pop()
                section_stack.append((level, title))
                start = document.find(ln, pos)
                end = start + len(ln)
                sections.append((title, "header",
                                {"level": level, "section_path": [t for _, t in section_stack]},
                                start, end))
                pos = end + 1
                i += 1
                continue

            if self._detect_content_type(ln) == "table":
                flush()
                start = document.find(ln, pos)
                tbl_lines = [ln]
                j = i + 1
                while j < len(lines) and self._detect_content_type(lines[j]) == "table":
                    tbl_lines.append(lines[j]); j += 1
                raw_block = "\n".join(tbl_lines)      # åŒç†ï¼šå…ˆ rawï¼Œå† strip å­˜å†…å®¹
                content = raw_block.strip()
                end = start + len(raw_block)
                sections.append((content, "table",
                                {"section_path": [t for _, t in section_stack]},
                                start, end))
                pos = end + 1
                i = j
                continue

            # æ™®é€šæ–‡æœ¬
            if not buf: cur_type = "text"
            buf.append(ln); i += 1

        flush()
        return sections


    # ---------- main segment ----------
    def segment(self, document: str, metadata: Optional[Dict] = None) -> List[DocumentSegment]:
        elements = self._split_by_structure(document)
        segs: List[DocumentSegment] = []
        seg_id = 0

        def add_segment(content: str, seg_type: str, start: int, end: int, extra_meta: Dict[str, Any]):
            nonlocal seg_id
            md = dict(metadata or {})
            md.update(extra_meta or {})
            segs.append(DocumentSegment(
                id=f"hyb_seg_{seg_id:04d}",
                content=content.strip(),
                segment_type=seg_type,
                start_pos=start,
                end_pos=end,
                metadata=md
            ))
            seg_id += 1

        for content, typ, meta, start, end in elements:
            if typ == "header":
                add_segment(content, "header", start, end, meta)
                continue

            if typ == "table":
                # Try to parse markdown table
                tbl = self._parse_md_table(content)
                if tbl:
                    title = meta.get("table_title") or (meta.get("section_path") or [])[-1] if meta.get("section_path") else None
                    lines = self._serialize_plain_table(tbl, title=title)
                    # chunk by rows to fit max_length
                    chunk, chunk_rows = [], []
                    cur_tokens = 0
                    # ensure first line could be a [Table] title
                    for ln in lines:
                        ln_tokens = self._token_len(ln)
                        new_len = (cur_tokens + ln_tokens)
                        if (chunk and new_len > self.max_length) or (len(chunk_rows) >= self.max_table_rows_per_chunk):
                            add_segment("\n".join(chunk), "table", start, end,
                                        {**meta, "serialization": "PLAIN",
                                         "row_range": (chunk_rows[0], chunk_rows[-1]) if chunk_rows else None})
                            chunk, chunk_rows, cur_tokens = [], [], 0
                        chunk.append(ln)
                        # row index: exclude the optional [Table] title
                        if not ln.startswith("[Table]"):
                            chunk_rows.append(len(chunk_rows))
                        cur_tokens = new_len
                    if chunk:
                        add_segment("\n".join(chunk), "table", start, end,
                                    {**meta, "serialization": "PLAIN",
                                     "row_range": (chunk_rows[0], chunk_rows[-1]) if chunk_rows else None})
                else:
                    # Fallback: treat as text
                    for sub in self._split_text_by_tokens(content):
                        add_segment(sub, "text", start, end, meta)
                continue

            # typ == "text"
            if self._token_len(content) <= self.max_length:
                add_segment(content, "text", start, end, meta)
            else:
                for sub in self._split_text_by_tokens(content):
                    add_segment(sub, "text", start, end, meta)

        # ---- Merge adjacent shorts (text + text / header + text) ----
        if not segs:
            return segs

        merged: List[DocumentSegment] = []
        buf = segs[0]
        for nxt in segs[1:]:
            can_merge = (
                (buf.segment_type == "text" and nxt.segment_type == "text") or
                (buf.segment_type == "header" and nxt.segment_type == "text")
            )
            if can_merge and (self._token_len(buf.content) + self._token_len(nxt.content) <= max(self.min_merge_tokens, self.max_length)):
                # merge
                new_content = (buf.content + "\n" + nxt.content).strip()
                buf = DocumentSegment(
                    id=buf.id,
                    content=new_content,
                    segment_type="text",
                    start_pos=buf.start_pos,
                    end_pos=nxt.end_pos,
                    metadata={**buf.metadata, **nxt.metadata}
                )
            else:
                merged.append(buf)
                buf = nxt
        merged.append(buf)

        return merged



class DocumentSegmenter:
    """Main document segmenter class"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.segmenter = self._initialize_segmenter()
        
    def _initialize_segmenter(self) -> BaseSegmenter:
        """Initialize based on configurationåˆ†æ®µå™¨"""
        method = self.config.get("split_method", "fixed").lower()
        max_length = self.config.get("max_segment_length", 1000)
        overlap_ratio = self.config.get("overlap_ratio", 0.1)
        
        if method == "fixed":
            return FixedLengthSegmenter(max_length, overlap_ratio)
        elif method == "semantic":
            similarity_threshold = self.config.get("similarity_threshold", 0.7)
            return SemanticSegmenter(
                max_length=max_length,
                similarity_threshold=similarity_threshold
            )
        elif method == "adaptive" or method == "hybrid":
            return HybridSegmenter(max_length, overlap_ratio)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„Segmentation method: {method}")
    
    def segment_document(self, document: str, metadata: Optional[Dict] = None) -> List[DocumentSegment]:
        """åˆ†æ®µæ–‡æ¡£"""
        logger.info(f"Starting document segmentationï¼Œlength: {len(document)} characters")
        segments = self.segmenter.segment(document, metadata)
        logger.info(f"Segmentation completedï¼ŒGenerate {len(segments)} segments")
        return segments
    
    def get_segment_statistics(self, segments: List[DocumentSegment]) -> Dict[str, Any]:
        """è·å–åˆ†æ®µç»Ÿè®¡ä¿¡æ¯"""
        if not segments:
            return {}
        
        lengths = [len(seg.content) for seg in segments]
        types = [seg.segment_type for seg in segments]
        
        stats = {
            "total_segments": len(segments),
            "avg_length": np.mean(lengths),
            "min_length": min(lengths),
            "max_length": max(lengths),
            "std_length": np.std(lengths),
            "type_distribution": {t: types.count(t) for t in set(types)}
        }
        
        return stats


# åœ¨ segmentation.py æ‰¹å¤„ç† CLI åŒä¸€æ–‡ä»¶ä¸­ï¼Œæ›¿æ¢/æ‰©å±• _pick_textï¼Œå¹¶æ–°å¢ markdown æ„é€ å‡½æ•°
def _get_ci(d: dict, key: str):
    """case-insensitive get: åœ¨å­—å…¸ d ä¸­æŒ‰ä¸åŒºåˆ†å¤§å°å†™æŸ¥æ‰¾ keyï¼Œè¿”å› (value, actual_key) æˆ– (None, None)"""
    key_l = key.lower()
    for k, v in d.items():
        if isinstance(k, str) and k.lower() == key_l:
            return v, k
    return None, None

def _rows_to_markdown_table(rows):
    if not isinstance(rows, list) or not rows:
        return None
    width = max(len(r) for r in rows if isinstance(r, list))
    norm = [(list(r) + [""] * (width - len(r))) if isinstance(r, list) else [""] * width for r in rows]
    header = norm[0] if norm else []
    if not any(header):
        header = [f"col_{i}" for i in range(width)]
        norm = [header] + norm
    sep = ["---"] * width
    lines = [
        "| " + " | ".join(str(x) for x in header) + " |",
        "| " + " | ".join(sep) + " |",
    ]
    for r in norm[1:]:
        lines.append("| " + " | ".join(str(x) for x in r) + " |")
    return "\n".join(lines)

def _pick_text(record: dict, candidate_keys: Tuple[str, ...]) -> Tuple[Optional[str], Optional[str]]:
    # 1) å…ˆæŒ‰å€™é€‰é”®ä¸åŒºåˆ†å¤§å°å†™åŒ¹é…ï¼ˆè¦†ç›– document / Document / documentText / DOCUMENT ç­‰ï¼‰
    # ä½†è·³è¿‡ç©ºå­—ç¬¦ä¸²ï¼Œå› ä¸ºæœ‰äº›æ•°æ®é›†ï¼ˆå¦‚TatQAï¼‰çš„document_textä¸ºç©ºä½†æœ‰å…¶ä»–å†…å®¹æº
    found_empty_document_text = False
    for k in candidate_keys:
        v, real_k = _get_ci(record, k)
        if isinstance(v, str):
            if v.strip():
                return v, real_k
            elif k.lower() in ("document_text", "document"):
                found_empty_document_text = True

    # 2) å¸¸è§å˜ä½“ï¼ˆé©¼å³°/ä¸‹åˆ’çº¿ï¼‰- åŒæ ·è·³è¿‡ç©ºå­—ç¬¦ä¸²
    for k in ["document_text", "documentText", "doc_text", "docText", "document"]:
        v, real_k = _get_ci(record, k)
        if isinstance(v, str):
            if v.strip():
                return v, real_k
            else:
                found_empty_document_text = True

    # 3) æå– paragraphsï¼šæ—¢æ”¯æŒ list[str] ä¹Ÿæ”¯æŒ list[{"text": ...}] æˆ– {"Text": ...}
    # æ”¯æŒç›´æ¥åœ¨recordä¸­æˆ–åœ¨metadataä¸­
    paras = record.get("paragraphs")
    if not paras and "metadata" in record:
        paras = record["metadata"].get("paragraphs")
    
    para_text = None
    if isinstance(paras, list):
        buf = []
        for it in paras:
            if isinstance(it, str) and it.strip():
                buf.append(it.strip())
            elif isinstance(it, dict):
                # æ”¯æŒå¤šç§æ–‡æœ¬å­—æ®µå
                for tk in ("text", "Text", "content", "Content"):
                    tv = it.get(tk)
                    if isinstance(tv, str) and tv.strip():
                        buf.append(tv.strip()); break
        if buf:
            para_text = "\n\n".join(buf)

    # 4) TAT-QA/DocFinQA è¡¨æ ¼ï¼š{"table": {"table": [[...]]}} æˆ– {"table": [[...]]}
    # æ”¯æŒç›´æ¥åœ¨recordä¸­æˆ–åœ¨metadataä¸­
    tbl = record.get("table")
    if not tbl and "metadata" in record:
        tbl = record["metadata"].get("table")
    
    rows = None
    if isinstance(tbl, dict) and isinstance(tbl.get("table"), list):
        rows = tbl["table"]
    elif isinstance(tbl, list):
        rows = tbl
    table_md = _rows_to_markdown_table(rows) if rows else None

    # 5) context å…œåº•ï¼ˆå¤§å°å†™å…¼å®¹ï¼‰
    # æ”¯æŒç›´æ¥åœ¨recordä¸­æˆ–åœ¨metadataä¸­
    ctx, real_ctx = _get_ci(record, "context")
    if not (isinstance(ctx, str) and ctx.strip()):
        if "metadata" in record:
            ctx, real_ctx = _get_ci(record["metadata"], "context")

    # 6) question å…œåº•
    # æ”¯æŒç›´æ¥åœ¨recordä¸­æˆ–åœ¨metadataä¸­
    q, real_q = _get_ci(record, "question")
    if not (isinstance(q, str) and q.strip()):
        if "metadata" in record:
            q, real_q = _get_ci(record["metadata"], "question")

    # 7) ç»„è£…ä¼˜å…ˆçº§ï¼šparagraphs + table + document/document_text + context + question
    # å¯¹äº TatQA ç­‰æ•°æ®é›†ï¼Œparagraphs å’Œ table æ˜¯ä¸»è¦å†…å®¹æº
    parts = []
    used = []
    
    # ä¼˜å…ˆä½¿ç”¨ paragraphsï¼ˆå¦‚æœå­˜åœ¨ä¸”éç©ºï¼‰
    if para_text:
        parts.append(para_text)
        used.append("paragraphs")
    
    # æ·»åŠ è¡¨æ ¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if table_md:
        parts.append(table_md)
        used.append("table(markdown)")
    
    # å¦‚æœä¸Šé¢æ²¡åŒ¹é…åˆ° document/document_textï¼Œå°è¯•æ”¶é›†
    if not parts or not any("document" in u for u in used):
        for k in ["document", "document_text", "documentText"]:
            v, real_k = _get_ci(record, k)
            if isinstance(v, str) and v.strip():
                parts.append(v)
                used.append(real_k)
                break
    
    # æ·»åŠ  contextï¼ˆå¦‚æœå­˜åœ¨ä¸”å‰é¢æ²¡æœ‰è¶³å¤Ÿå†…å®¹ï¼‰
    if ctx and len(parts) < 2:
        parts.append(ctx)
        used.append(real_ctx)
    
    # æœ€åå…œåº•ï¼šquestionï¼ˆåªåœ¨æ²¡æœ‰å…¶ä»–å†…å®¹æ—¶ä½¿ç”¨ï¼‰
    if q and not parts:
        parts.append(q)
        used.append(real_q)

    # å¦‚æœåªæœ‰ paragraphs ä½†å†…å®¹ä¸ºç©ºï¼Œç›´æ¥è¿”å› paragraphs
    if para_text and not table_md:
        return para_text, "paragraphs"
    
    # å¦‚æœåªæœ‰ table ä½†æ²¡æœ‰å…¶ä»–å†…å®¹
    if table_md and not para_text:
        return table_md, "table(markdown)"
    
    # ç»„åˆå¤šä¸ªéƒ¨åˆ†
    if parts:
        return "\n\n".join(parts), "+".join(used) if used else "composed"

    return None, None

def _ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)


def run_batch():
    parser = argparse.ArgumentParser("Batch segmenter for data/*/**/train.jsonl â†’ data/segmented_data mirror")
    parser.add_argument("--data-root", default="data", help="æ ¹ç›®å½•ï¼Œé€’å½’å¯»æ‰¾ train.jsonl")
    parser.add_argument("--out-root", default="data/segmented_data", help="è¾“å‡ºé•œåƒæ ¹ç›®å½•")
    parser.add_argument("--glob-name", default="train.jsonl", help="ç›®æ ‡æ–‡ä»¶åï¼ˆé»˜è®¤åªå¤„ç† train.jsonlï¼‰")
    # åˆ†æ®µé…ç½®ï¼ˆå¤ç”¨ DocumentSegmenter çš„é…ç½®é”®ï¼‰
    parser.add_argument("--split-method", default="hybrid", choices=["fixed", "semantic", "hybrid", "adaptive"])
    parser.add_argument("--max-segment-length", type=int, default=1000)
    parser.add_argument("--overlap-ratio", type=float, default=0.1)
    parser.add_argument("--similarity-threshold", type=float, default=0.7, help="ç”¨äº semantic")
    parser.add_argument("--semantic-breaks", action="store_true", help="hybrid ä¸­æ–‡æœ¬åˆ‡ç‰‡æ—¶å¯ç”¨è¯­ä¹‰æ–­ç‚¹")
    parser.add_argument("--semantic-device", default="cuda", help="sentence-transformers è®¾å¤‡")
    # è¾“å…¥æ–‡æœ¬å­—æ®µå€™é€‰
    parser.add_argument(
        "--text-keys",
        default="document,text,content,context,passage,article,body,report,raw_text",
        help="é€—å·åˆ†éš”çš„å€™é€‰å­—æ®µåï¼ŒæŒ‰é¡ºåºåŒ¹é…"
    )
    parser.add_argument("--keep-input", action="store_true", help="åœ¨è¾“å‡ºä¸­ä¿ç•™åŸå§‹è¾“å…¥ recordï¼ˆå¯èƒ½å¾ˆå¤§ï¼‰")
    parser.add_argument("--fail-fast", action="store_true", help="é‡åˆ°è§£æ/åˆ†æ®µé”™è¯¯ç«‹å³é€€å‡º")
    args = parser.parse_args()

    data_root = Path(args.data_root).resolve()
    out_root = Path(args.out_root).resolve()
    candidate_keys = tuple([k.strip() for k in args.text_keys.split(",") if k.strip()])

    # æ„é€ åˆ†æ®µå™¨
    seg_config = {
        "split_method": args.split_method,
        "max_segment_length": args.max_segment_length,
        "overlap_ratio": args.overlap_ratio,
        "similarity_threshold": args.similarity_threshold,
        "semantic_breaks": args.semantic_breaks,
        "semantic_device": args.semantic_device,
    }
    segmenter = DocumentSegmenter(seg_config)

    # éå†æ‰€æœ‰ train.jsonl
    total_files = 0
    for root, _, files in os.walk(data_root):
        if args.glob_name not in files:
            continue

        in_path = Path(root) / args.glob_name
        rel_dir = Path(os.path.relpath(root, data_root))  # ç›¸å¯¹ data_root çš„è·¯å¾„
        mirror_dir = out_root / rel_dir
        _ensure_dir(mirror_dir)

        out_path = mirror_dir / args.glob_name.replace(".jsonl", ".segmented.jsonl")
        stats_path = mirror_dir / args.glob_name.replace(".jsonl", ".segmented.stats.json")

        print(f"[SEG] {in_path} â†’ {out_path}")

        n_lines = 0
        n_ok = 0
        n_skip = 0
        n_err = 0
        total_segments = 0
        example_keys = set()

        with in_path.open("r", encoding="utf-8") as fin, out_path.open("w", encoding="utf-8") as fout:
            for lineno, line in enumerate(fin, start=1):
                line = line.strip()
                if not line:
                    n_skip += 1
                    continue

                n_lines += 1
                try:
                    record = json.loads(line)
                except Exception as e:
                    n_err += 1
                    if args.fail_fast:
                        raise
                    # å†™å…¥é”™è¯¯å ä½ï¼ˆå¯é€‰ï¼‰
                    err_obj = {
                        "source": {
                            "path": str(in_path.relative_to(data_root)),
                            "line_no": lineno,
                        },
                        "error": f"JSONDecodeError: {str(e)}"
                    }
                    fout.write(json.dumps(err_obj, ensure_ascii=False) + "\n")
                    continue

                text, used_key = _pick_text(record, candidate_keys)
                if used_key:
                    example_keys.add(used_key)

                if not text:
                    n_skip += 1
                    warn_obj = {
                        "source": {
                            "path": str(in_path.relative_to(data_root)),
                            "line_no": lineno,
                            "id": record.get("id")
                        },
                        "warn": "no_text_field_found",
                        "tried_keys": candidate_keys
                    }
                    fout.write(json.dumps(warn_obj, ensure_ascii=False) + "\n")
                    continue

                try:
                    meta = {
                        "source_relpath": str(in_path.relative_to(data_root)),
                        "line_no": lineno,
                        "record_id": record.get("id"),
                        "used_key": used_key
                    }
                    segs = segmenter.segment_document(text, metadata=meta)
                    total_segments += len(segs)

                    out_obj = {
                        "source": meta,
                        "segments": [asdict(s) for s in segs],
                        "segmentation_config": {
                            "method": args.split_method,
                            "max_segment_length": args.max_segment_length,
                            "overlap_ratio": args.overlap_ratio,
                            "semantic_breaks": args.semantic_breaks
                        }
                    }
                    if args.keep_input:
                        out_obj["input"] = record

                    fout.write(json.dumps(out_obj, ensure_ascii=False) + "\n")
                    n_ok += 1

                except Exception as e:
                    n_err += 1
                    if args.fail_fast:
                        raise
                    err_obj = {
                        "source": meta,
                        "error": f"SegmentationError: {str(e)}"
                    }
                    fout.write(json.dumps(err_obj, ensure_ascii=False) + "\n")

        # å†™æ–‡ä»¶çº§åˆ«ç»Ÿè®¡
        stats = {
            "input_file": str(in_path),
            "output_file": str(out_path),
            "lines_total": n_lines,
            "lines_segmented": n_ok,
            "lines_skipped_no_text": n_skip,
            "lines_error": n_err,
            "avg_segments_per_line": (total_segments / n_ok) if n_ok else 0.0,
            "used_text_keys": sorted(list(example_keys)),
            "segmenter_config": seg_config
        }
        with stats_path.open("w", encoding="utf-8") as fstats:
            json.dump(stats, fstats, ensure_ascii=False, indent=2)

        total_files += 1

    print(f"[DONE] processed files: {total_files}  (root={data_root})")


if __name__ == "__main__":
    run_batch()